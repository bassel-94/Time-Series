---
title: "Homework"
author: "Bassel MASRI & Andrei CHIRITA"
date: "1/21/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE, warning=FALSE, echo=FALSE}
#https://fred.stlouisfed.org/series/CPIAUCNS
#https://fred.stlouisfed.org/series/TRFVOLUSM227NFWA
#-- clean environment and load libraries
rm(list=ls())
if(!require(MTS)) install.packages("MTS")
library(ggplot2)
library(tidyverse)
library(zoo)
library(tseries)
library(dplyr)
library(forecast)
library(gridExtra)
library(scales)
library(anomalize)
library(vars)
```
\newpage
\setcounter{tocdepth}{2}
\pagenumbering{arabic}
\tableofcontents
\newpage

# Exercise 1

The purpose of this exercise is to simulate a path of length $T = 100$ from the VAR(1) model : 

$$
X_t = A_n X_{t-1} + \epsilon_t
$$

Where $\epsilon_t$ is a standard multivariate gaussian white noise.

To demonstrate the parameters of the problem, we will take the simple example of $n=2$, that is a stationary bivariate VAR(1) model which has the following form :

$$
\left(\begin{array}{c} 
X_{1t}\\ 
X_{2t}
\end{array}\right)
= 
\left(\begin{array}{c} 
c_1\\ 
c_2
\end{array}\right)
+
\left(\begin{array}{cc} 
A_{1,1} & A_{1,2}\\ 
A_{2,1} & A_{2,2}
\end{array}\right)
\times
\left(\begin{array}{c} 
X_{1t-1} \\ 
X_{2t-1}
\end{array}\right)
+
\left(\begin{array}{c} 
\epsilon_{1t} \\ 
\epsilon_{2t}
\end{array}\right)
$$ 
In our case, we have the following parameters; $A_{n}(i,i) = \frac{1}{2}$, $A_{n}(i,i+1) = \frac{1}{5}$, and $A_{n}(i,j) = 0$. For the specific case of $n=2$, this enables us to rewrite the model that we need to estimate as follows:

$$
\left(\begin{array}{cc} 
X_{1t}\\ 
X_{2t}
\end{array}\right)
= 
\left(\begin{array}{cc} 
c_1\\ 
c_2
\end{array}\right)
+
\left(\begin{array}{cc} 
0.5 & 0.2\\ 
0 & 0.5
\end{array}\right)
\times
\left(\begin{array}{cc} 
X_{1t-1} \\ 
X_{2t-1}
\end{array}\right)
+
\left(\begin{array}{cc} 
\epsilon_{1t} \\ 
\epsilon_{2t}
\end{array}\right)
$$ 

The steps are :

* Define the parameters of the problem
* Generate VAR(1) model
* Estimate VAR(1) model
* Calculate error on the estimated VAR(1) model.

We will compute the above steps using the following code chunk:

```{r}
#-- define function that simulates var models
simulate_var = function(n, T) {
  
  #-- set seed for reproducible results
  set.seed(123)
  error=rep(0, length(n))
  
  for (l in seq_along(n)){
    p=n[l]
    A=0.5*diag(p)
    X=matrix(0,nrow=p ,ncol=T)
    
    for (i in 1:(p-1)){
      A[i,i+1]=0.2
    }
    
    #-- Generate a VAR model of dimension n[l] with T obs
    for (j in 2:T){
      X[,j]=A%*%X[,(j-1)]+t(t(rnorm(p)))
    }
    
    #-- Estimate a VAR model of dimension n[l] from T obs
    Estim=VAR(t(X),1)
    A_hat=Estim$Phi
    
    #-- Study the estimation error
    B=A_hat-A
    error[l]=sqrt(max(abs(eigen(t(B)%*%B)$values)))
  }
  return(error)
}
```

```{r, warning=FALSE, include=FALSE}
#-- test for T=c(100,200,1000)
n = c(2,5,10,20)
error_T100 = simulate_var(n,100)
error_T500 = simulate_var(n,200)
error_T1000 = simulate_var(n,1000)
```

```{r, include=FALSE, echo = FALSE}
#-- group results in a data frame and produce a column of indices to plot against
df = data.frame("T_100" = error_T100, 
                "T_500" = error_T500, 
                "T_1000" = error_T1000) %>% 
  rowid_to_column() %>%
  gather(key = "T_values", value = "Errors", -rowid)
```

We then compute the error $n=\{2,5,10,20\}$ for the following path lengths; $T=\{100,500,1000\}$ and we get the following plot: 

```{r, echo=FALSE,fig.align='center', fig.width=4, fig.height=3, fig.cap="The variation of error as a function of simulation length T and number of covariates n"}
#-- plot results
ggplot(df, aes(x=rowid, y = Errors, color = T_values)) + 
  geom_line() + geom_point() + theme_bw() + 
  ggtitle("Errors as a function of T and n") + 
  scale_x_discrete(name ="Values of n", limits=c("2","5","10", "20")) +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.6), 
        legend.title = element_text(size = 8, face = "bold"),
        legend.text = element_text(size = 6),
        axis.text=element_text(size=6),
        axis.title=element_text(size=8,face="bold"))
```

Upon examining the plot of the squared $\ell_2$ error term, we notice a consistent increase in the error with respect to the numbers of covariates. When n becomes large, the error value increases. This is expected because the more covariates we have to estimate, the more error we introduce in the model. 

As for the path length $T$, we notice that the smallest error values are for path $T=1000$. This is due to the fact that we have more observations in the time series that enables us to estimate the coefficients more accuratly. To put differently, with more *information* to estimate the model's parameters, we are able to achieve more accurate results.

\newpage

# Exercise 2 

## Goal of the Analysis

Our analysis has as purpose to find out what are the main influences on gasoline prices. More specifically we try to put the evolution of gasoline prices in the last 26 year in the context of 2 indicators: the vehicle related mobility and crude oil prices.

In order to do that we use monthly data from the United States. We chose monthly data because it is easier to interpret in economically. We decided on using data from the United States because the U.S.A. is the largest economy in the world and one of the largest crude oil producers and special rules around the export of oil products. Thus we can consider that the U.S. has an oil economy that is both U.S.A.-centered and at the same time representative of worldwide trends in oil and gasoline prices. 

## Description of the data

During our analysis we use 5 time series, 4 related to the evolution of the prices and consumption and gasoline and diesel in the United States of America and 1 general economic indicator (The CPI). The series are:

* U.S. All Grades All Formulations Retail Gasoline Prices (referred bellow as gasoline prices): It is a general value of gasoline prices in the United States, the series is downloaded from the U.S. Energy Information Administration (EIA). The values of this series are expressed in Dollars per Gallon.

* U.S. No 2 Diesel Retail Prices (referred bellow as diesel prices): It is a general value of diesel prices in the United States, the series is downloaded from EIA. The values of the series are expressed in Dollars per Gallon.

* Cushing, OK WTI Spot Price FOB (referred bellow as crude oil price): It is a general value of the price of crude oil in the United States, the series is downloaded from EIA and its values are expressed in Dollars per Barrel.

* Vehicle Miles Traveled: It represents a measure of the vehicle-related mobility in the United States, the series is downloaded from the site of the Federal Reserve Bank of St. Louis. The unit of measurement is: millions of miles.

* Consumer Price Index for All Urban Consumers: All Items in U.S. City Average: This is a measure of CPI, the downloaded data is an index with the base in the early 1980's. We needed this series because we are dealing with prices that need to be adjusted with inflation in order to allow for comparisons between distant time periods. This series was downloaded from the site of the Federal Reserve Bank of the United States.

In order to solve the first question, we choose to analyze the time series Miles and Crude.

## Loading and Preparing the Data

On the first step we had to load the data from the csv files it was stored on. Then we made sure that the dates are expressed in a format that was comprehensible for R.
On this step we also re-expressed the CPI in order for it's base to be the month of April of 1994. The change of basis was done by dividing all the values in the CPI series with the value for April 1994.
Next we "assembled the data sets" and extracted the year for plotting, we also reshaped the data in a long format, again for plotting purposes.

```{r, include = FALSE, echo=FALSE}
data<-read.csv("serie completa.csv",sep=",",header=TRUE)
dates <- seq(ISOdate(1994,4,1), by = "month", length.out = 321)
data$Date<-dates
data<-data[-321,]
CPI<-read.csv("CPIAUCNS.csv",sep=",",header=TRUE)
CPI<-CPI[-321,]
Road_travel<-read.csv("TRFVOLUSM227NFWA.csv")
```

```{r, include = FALSE, echo=FALSE}
CPI1<-data.frame(DATE=CPI[,1],CPI[,2]/CPI[1,2])
```

```{r,include = FALSE, echo=FALSE}
data<-data.frame(data,Miles_Travelled=Road_travel[,2])
```

```{r,include = FALSE, echo=FALSE}
data2<-reshape(data, 
        direction = "long",
        varying = list(names(data)[2:5]),
        v.names = "Value",
        idvar = c("Date"),
        timevar = "Type",
        times = c("Diesel","Gasoline","Crude","Miles"))
years=as.numeric(format(as.Date(data2$Date, format="%d/%m/%Y"),"%Y"))
years<-substr(years,3,4)
years<-factor(years,levels = c("94","95","96","97","98","99","00",
                               "01","02","03","04","05","06","07","08","09",
                               "10","11","12","13","14","15","16","17","18","19","20"))
data2<-data.frame(data2,year=years)
```

## Yearly and Monthly Evolution of the Series

### Yearly Evolution

The first step of our exploratory data analysis of the time series Crude and Miles is to produce a plot showing the evolution of Crude oil prices as well as the miles traveled over time.  

```{r, warning=FALSE, echo=FALSE,fig.align="center", fig.width=8, fig.height=4, fig.cap="Yearly evolution of crude oil prices and miles traveled"}

p1 = ggplot(data2[data2$Type=="Crude",])+
  aes(x=as.factor(year),y=Value)+
  geom_boxplot()+
  stat_summary(fun=median, geom="smooth", aes(group=1))+
  ggtitle("Yearly Evolution of Crude Oil Prices")+
  xlab("Year")+
  ylab("Price in dolars") + theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p2 = ggplot(data2[data2$Type=="Miles",])+
  aes(x=as.factor(year),y=Value)+
  geom_boxplot()+
  stat_summary(fun=median, geom="smooth", aes(group=1))+
  ggtitle("Yearly Evolution of Miles Travelled per Month")+
  xlab("Year")+
  ylab("Miles") + theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(p1,p2, ncol = 2)
```

The plots above show the values for each year as boxplots with the blue line uniting the medians of each year. For the crude oil prices we can observe that prices increased steadily from 2003 to 2008 when they had a sudden drop due to the financial and economic crisis of that year. The drop have been recovered from 2011 to 2014 prices and remained on a plateau just bellow 100$ per barrel. A significant drop of price is clearly observed in the end of 2019 to early 2020 which can be interrupted by the COVID-19 pandemic. From a variability point of view, three years stand out; 2007, 2008 and 2009, this are years when the price grew and declined very rapidly. The number of miles traveled per month had a similar variability each year and most years also have an extreme low-value. The series has an ascending trend that was "smoothed" by the crisis of 2008 and somewhat reversed (probably temporarily) by the COVID-19 pandemic.

### Monthly Evolution

In the following plots we can observe the monthly evolution of the data sets, we had to plot them separately because of the different units of measurement. The evolution of diesel and gasoline prices was very similar and it basically followed he same movement trajectory as that of the crude oil.

```{r, warning=FALSE, echo=FALSE, fig.width=10, fig.height=4, fig.cap="Monthly evolution of crude oil prices and miles traveled"}
p2 = ggplot(data2[data2$Type=="Crude",])+
  aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  ggtitle("Crude Oil Prices")+
  xlab("Year")+
  ylab("Price in dolars (1994)")+
  geom_line(aes(y=rollmean(Value, 12, na.pad=TRUE)),colour="blue") + theme_bw()

p3 = ggplot(data2[data2$Type=="Miles",])+
  aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  geom_line(aes(y=rollmean(Value, 12, na.pad=TRUE)),colour="blue") + theme_bw() +
  ggtitle("Miles Travelled per Month")+
  xlab("Year")+
  ylab("Miles")

grid.arrange(p3,p2, ncol = 2)
```

On the plot of the prices of crude oil  we added a blue line representing a moving average with basis 12. It has a similar trajectory to the non-smoothed series. 
For the traveled miles data we can observe a clear seasonality with travel declining around February. In such case, smoothing using moving averages has a greater impact allowing us to see the trend.

## Adjusting for inflation

The next step of our analysis would be to adjust the series for inflation. This step is essential in order to make the prices of different time periods comparable as inflation can induce a trend where there is actually none. The adjustment was done by dividing the price time series by the index of the CPI corresponding to each month. The basis for the adjustment was the month of April of 1994 which would give the following plots for the monthly evolution of Miles traveled and Crude oil prices.

```{r, warning=FALSE, echo=FALSE, fig.width=8, fig.height=4, fig.cap="Monthly evolution of crude oil and Diesel & Gasoline prices after adjustment for inflation"}
data3<-data.frame(Date=data$Date,
                 Diesel=data$U.S..No.2.Diesel.Retail.Prices..Dollars.per.Gallon./CPI1[,2],
                 Gasoline=data$U.S..All.Grades.All.Formulations.Retail.Gasoline.Prices..Dollars.per.Gallon./CPI1[,2],
                 Crude=data$Cushing..OK.WTI.Spot.Price.FOB..Dollars.per.Barrel./CPI1[,2],
                 Miles=data$Miles_Travelled)

data4<-reshape(data3, 
        direction = "long",
        varying = list(names(data3)[2:5]),
        v.names = "Value",
        idvar = c("Date"),
        timevar = "Type",
        times = c("Diesel","Gasoline","Crude","Miles"))

data4<-data.frame(data4,year=as.numeric(format(as.Date(data4$Date, format="%d/%m/%Y"),"%Y")))

p2 = ggplot(data4[data4$Type=="Crude",])+
  aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  ggtitle("Crude Oil Prices",subtitle = "Adjusted for inflation")+
  xlab("Year")+
  ylab("Price in dolars (1994)")+
  geom_line(aes(y=rollmean(Value, 12, na.pad=TRUE)),colour="blue") + theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p2
```

Indeed, the monthly evolution of adjusted crude oil prices shows similar results to the unadjusted one. However, the prices in the last years registered in the times series are lower and thus closer to the ones of the early years of the time series.

```{r}
data4<-data.frame(data4,month=as.numeric(format(as.Date(data4$Date, format="%d/%m/%Y"),"%m")))
RA_Miles<-rollmean(data3$Miles,12)

for ( i in 1:nrow(data4)){
  if(data4$month[i] %in% c(12,1,2)){
  data4$season[i]<-"winter"
}
if (data4$month[i] %in% c(3,4,5)){
  data4$season[i]<-"spring"
}
if(data4$month[i] %in% c(6,7,8)){
  data4$season[i]<-"summer"
}
if (data4$month[i] %in% c(9,10,11)){
  data4$season[i]<-"autumn"
}
}
# ggplot(data4[data4$Type=="Miles",])+
#   aes(x=as.factor(season),y=Value)+
#   geom_boxplot()+
#   stat_summary(fun=median, geom="smooth", aes(group=1))+
#   ggtitle("Monthly Miles Travelled")+
#   xlab("Month")+
#   ylab("Miles Travelled")
```

## Time Series decomposition and analysis

Time series decompostion is an essential step for the exploratory data analysis. It gives rich insight on the different elements of the series when it comes to trend, seasonality and the remainder. The time decomposition method that we chose is "STL" which stands for “Seasonal and Trend decomposition using Loess,” while Loess is a method for estimating nonlinear relationships. The following plot shows the time decomposition of the univariate time series "Crude oil" to better visualize how it works.

In this section, we will perform a time series decomposition of the crude oil and the miles times series to further explore their trend and seasonality.

```{r, warnings = FALSE, fig.align='center', fig.width=12, fig.height=6, echo = FALSE, fig.cap="Time series decomposition displaying seasonality and trend"}
df = data3 %>%
  mutate(Date = as.POSIXct(Date)) %>% 
  remove_rownames() %>% 
  as_tibble()

#-- Time series decomposition of Crude, Miles, Gasoline and Diesel
d1 = df %>% time_decompose(Crude, method = "stl", frequency = "auto", trend = "auto", message = FALSE)
d2 = df %>% time_decompose(Miles, method = "stl", frequency = "auto", trend = "auto", message = FALSE)
d3 = df %>% time_decompose(Gasoline, method = "stl", frequency = "auto", trend = "auto", message = FALSE)
d4 = df %>% time_decompose(Diesel, method = "stl", frequency = "auto", trend = "auto", message = FALSE)

#-- plots for only crude and miles time series and arrange in a grid
p2 = ggplot(d1, aes(x = Date, y = season)) +
  geom_line(color = "steelblue", size = 1) + theme_bw() + ggtitle("Seasonality") + 
  scale_x_datetime(date_labels = "%Y", breaks = date_breaks("1 year"), labels=date_format('%Y')) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p3 = ggplot(d1, aes(x = Date, y = trend)) +
  geom_line(color = "steelblue", size = 1) + theme_bw() + ggtitle("Trend") + 
  scale_x_datetime(date_labels = "%Y", breaks = date_breaks("1 year"), labels=date_format('%Y')) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#-- do plots and arrange in a grid
p5 = ggplot(d2, aes(x = Date, y = season)) +
  geom_line(color = "steelblue", size = 1) + theme_bw() + ggtitle("Seasonality") + 
  scale_x_datetime(date_labels = "%Y", breaks = date_breaks("1 year"), labels=date_format('%Y')) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p6 = ggplot(d2, aes(x = Date, y = trend)) +
  geom_line(color = "steelblue", size = 1) + theme_bw() + ggtitle("Trend") + 
  scale_x_datetime(date_labels = "%Y", breaks = date_breaks("1 year"), labels=date_format('%Y')) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(p2,p5,p3,p6, nrow = 2, ncol = 2)
```

Upon examining the crude oil plots, we notice the unexistance of a linear trend, hence indicating non stationarity in the crude oil prices. In addition, we notice that the seasonal trend is slightly irregular throughout the year. The upper and lower bound of the seasonal component hints that it is statistically insignificant compared to the actual bounds of the crude oil prices.

The Miles times series, however, shows some more structure. The seasonality is non-negligable and a linear trend is visible. We can zoom in further on the seasonality effect of this variable by showing a box plot of the mean of all years of data throughout each month to obtain the following plot.

```{r, echo=FALSE, warning=FALSE, fig.width=6, fig.height=4, fig.cap="Seasonality of Miles"}
for ( i in 1:nrow(data4)){
  if(data4$month[i] %in% c(12,1,2)){
  data4$season[i]<-"winter"
}
if (data4$month[i] %in% c(3,4,5)){
  data4$season[i]<-"spring"
}
if(data4$month[i] %in% c(6,7,8)){
  data4$season[i]<-"summer"
}
if (data4$month[i] %in% c(9,10,11)){
  data4$season[i]<-"autumn"
}
}

to_plot<-aggregate(Value~Type+year+season,data=data4,mean)
to_plot$means<--1
to_plot$s_index<-0
season<-c("winter","spring","summer","autumn")
for (i in unique(to_plot$Type)){
  for (j in unique(season)){
    to_plot[to_plot$Type==i&to_plot$season==j,]$means<-mean(to_plot[to_plot$Type==i&to_plot$season==j,"Value"])
    to_plot[to_plot$Type==i&to_plot$season==j,]$s_index<-(max(to_plot[to_plot$Type==i,"s_index"])+1):(max(to_plot[to_plot$Type==i,"s_index"])+nrow(to_plot[to_plot$Type==i&to_plot$season==j,]))
  }
}
ggplot(to_plot[to_plot$Type=="Miles",])+
  aes(x=s_index,y=Value,colour=season)+
  geom_line(size = 1)+
  geom_line(aes(y=means,colour=season), size = 1.5)+
  ggtitle("Monthly Miles Travelled")+
  xlab("Month")+
  ylab("Miles Travelled") + theme_bw()
```

Indeed, we find that during winters, people are less likely to be traveling due to harsh weather conditions. In contrast, summers tend to be more active and we can see a clear increase of miles traveled in hotter periods.

## ACF, PACF and the white noise assumption

One of the most important assumptions of any time series modeling is *random noise*. Therefore, we will explore the random component of the time series decomposition in order to make sure it satisfies the random noise assumptions which are the following :

* Little to no autocorrelation (i.e.  $\mathbb{E} (\varepsilon_t \times \varepsilon_\tau) = 0$ $\forall t,\tau \in \mathbb{Z}$ given that $ \tau \neq t$ )
* Normal distribution with $\mathbb{E} (\varepsilon_t) = 0$ and $\mathbb{E} (\varepsilon_t^2) = \sigma^2$

To check these assumptions, we will first plot the autocorrelation of the random components of Crude and Miles in the figure below.

```{r, warnings = FALSE, fig.align='center', fig.width=6, fig.height=3, echo = FALSE, fig.cap="Autocorrelation of the white noise processes"}
p1 = ggAcf(ts(d1$remainder)) + ggtitle("ACF of Crude's random component ") + theme_bw() + theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5))
p2 = ggAcf(ts(d2$remainder)) + ggtitle("ACF of Miles' random component") + theme_bw() + theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5))
grid.arrange(p1,p2, ncol = 2)
```

We can clearly discern some structure in the beginning of the autocorrelation plots of the white noise processes for each of the time series at hand. This is an indicator of non stationarity that should be handled before proceeding to do any modeling. 

Similarly, the ACF and PACF plots below indicate high autocorrelation with respect to their lag, further confirming our non stationarity hypothesis throughout the exploratory data analysis.

Another measure that we must check is the partial autocorrelation of the series, as we can see the partial autocorelations tend not to decrease under the representativity threshold after the first lag. This big partial autocorrelation in later lags suggests that the series are not stationary.

```{r, warnings = FALSE, fig.align='center', fig.width=6, fig.height=4, echo = FALSE, fig.cap="Autocorrelation of the covariates and their random component"}
p1 = ggPacf(ts(data3[,5])) + ggtitle("PACF of Miles") + theme_bw() + theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5))
p2 = ggPacf(ts(data3[,4])) + ggtitle("PACF of Crude") + theme_bw() + theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5))
p3 = ggAcf(ts(data3[,5])) + ggtitle("ACF of Miles") + theme_bw() + theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5))
p4 = ggAcf(ts(data3[,4])) + ggtitle("ACF of Crude") + theme_bw() + theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5))
grid.arrange(p3,p4,p1,p2)
```

Another way of checking stationary of time series is the Augmented Dickey-Fuller test which we performed on all the variables of the time series.

```{r, echo=FALSE, warning = FALSE, include = FALSE}
to_table<-data.frame(Series_name=0,Lag_order=0,p_value=0)
for (i in 2:ncol(data3)){
  to_table[i-1,1]<-colnames(data3)[i]
  to_table[i-1,2]<-1
  test<-adf.test(data3[,i],alternative="stationary",k=1)
  to_table[i-1,3]<-test$p.value
}
to_table[4,1]<-"Miles Rolling Averages"
to_table[4,2]<-12
test<-adf.test(data3$Miles,alternative="stationary",k=12)
to_table[4,3]<-test$p.value
```

```{r, echo=FALSE, warning=FALSE}
knitr::kable(to_table, 
  caption = 'P value of the statistic Augmented Dickey-Fuller', 
  col.names = c("Series name", "Lag order", "p-value")
)
```

From the table above we see that the results of the p-value of the Augmented Dickey-Fuller test for the series are generally above 0.05 which means that our series are non-stationary. In regard to the Miles variable the test for lag one says that the series is stationary, but this effect is due to the seasonal patterns of travel in the United States. If we were to perform the test for a lag of 12 we would be able to discover the non-stationarity of the series. In order to fix the stationarity of the Miles series we replaced it with its rolling averages with a base 12. As rolling averages are computed as an average over 12 values of the series we can expect them to be non-stationary (a fact that we can also observe in the plot above).

## Removing Non-Stationarity

Our conclusion from the exploratory data analysis indicates that the time series we are studying is, indeed, a non-stationary process. A non-stationary process, by definition contains a trend that has a mean growing around a fixed trend, which is constant and independent of time.

However, a non-stationary time series can be transformed into a stationary process by differencing (i.e. subtracting $X_{t-1}$ from $X_t$).
Therefore, in this section, we will procees by taking the difference : 

$$
X_t \rightarrow X_t - X_{t-1}
$$ 

After doing so, We plot the auto correlation again to make sure we removed the non-stationarity and get the following results:

```{r, warnings = FALSE, fig.align='center', fig.width=6, fig.height=3, echo = FALSE, fig.cap="Autocorrelation of the covariates after differentiating"}
#-- differentiate to remove non-stationnarity
data5<-data.frame(Date=1:319)
data5$Date<-data3$Date[-1]
data3$RA_Miles<-rollmean(data3$Miles,12,fill=NA)
data5<-data.frame(data5,apply(data3[,2:6],MARGIN = 2,FUN = diff))
RA_Miles_stat<-diff(RA_Miles)

p3 = ggAcf(ts(data5$Crude)) + ggtitle("ACF of Crude") + theme_bw() + theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5))
p4 = ggAcf(ts(RA_Miles_stat)) + ggtitle("ACF of Miles") + theme_bw() + theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5))
grid.arrange(p3,p4, ncol = 2)
```

After examining the autocorrelation plots for the second time after differentiating, we can see that we get, to certain extent, a nicer stationary time series that can now use to build our ARMA model in the next step.

## Fitting ARMA Models

### Crude

In order to choose the orders of the model (p and q) we implemented a cross-validation algorithm that tries 25 models (we tried p from 1 to 5 and q from 1 to 5 considering all possible combinations). We decided on models using just one MA or AR components in order to have a more parcimonious/sparse? model since that would make the results more interpretable.

```{r, warnings = FALSE,include=FALSE}
errors<-data.frame(AR=1,MA=1,ERR=1)
k<-1
for (i in 1:5){
  for(j in 1:5){
    model<-arma(data5$Crude,lag=list(ar=c(i),ma=c(j)))
    errors[k,1]<-i
    errors[k,2]<-j
    errors[k,3]<-model$css
    k<-k+1
  }
}
```

```{r, echo=FALSE,fig.align='center', fig.width=5, fig.height=4, fig.cap="Cross validation error against the order of AR and MA processes for Crude oil prices time series"}
ggplot(errors)+aes(x=AR,y=ERR,colour=as.factor(MA))+geom_point(size = 2, alpha = 0.5)+ylab("Error") + 
  theme_bw() + ggtitle("Cross validation error against order of the models") + 
  xlab("AR process order")+ ylab("Cross Validation Error")
```

The plot above shows the empirical error of the models. Based on it's output we chose a model with AR=2 and MA=1.

```{r, warnings = FALSE,include=FALSE}
model<-arma(data5$Crude,lag=list(ar=c(2),ma=c(1)))
```

```{r}
shapiro.test(na.omit(model$residuals))
qqnorm(model$residuals)
plot(density(na.omit(model$residuals)))
```

```{r, fig.align='center', fig.width=8, fig.height=6, echo = FALSE, warning=FALSE, fig.cap="Fitted ARMA models on the time series Crude"}
#print(" The coefficients of the fitted model are: ")
#model$coef
to_plot<-data.frame(Date=data5$Date,Original_Series=data5$Crude,Fitted_Series=model$fitted.values)
Fitted_Series<-rep(NA,nrow(data3))
Fitted_Series[length(model$fitted.values)-length(na.omit(model$fitted.values))]<-data3$Crude[length(model$fitted.values)-length(na.omit(model$fitted.values))]
for(i in (length(model$fitted.values)-length(na.omit(model$fitted.values))):(nrow(data3)-1)){
  Fitted_Series[i+1]<-Fitted_Series[i]+model$fitted.values[i+1]
}
to_plot<-reshape(to_plot, 
        direction = "long",
        varying = list(names(to_plot)[2:3]),
        v.names = "Value",
        idvar = c("Date"),
        timevar = "Type",
        times = c("Original Series","Fitted Series"))
p1=ggplot(to_plot)+aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  xlab("Monthly Change")+
  ggtitle("Crude Oil Price",subtitle = "First difference") + theme_bw()

to_plot<-data.frame(Date=data3$Date,Original_Series=data3$Crude,Fitted_Series=Fitted_Series)
to_plot<-reshape(to_plot, 
        direction = "long",
        varying = list(names(to_plot)[2:3]),
        v.names = "Value",
        idvar = c("Date"),
        timevar = "Type",
        times = c("Original Series","Fitted Series"))

p2=ggplot(to_plot)+aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  xlab("Monthly Price")+
  ggtitle("Crude Oil Price",subtitle = "Real Vs. Fitted") + theme_bw()

grid.arrange(p1,p2)
```

We can observe from the plots above our model have been successful at capturing the evolution of both the differentiated series and original series. In contrast, our model fails to fully take into account is the variance of the series, it has a lower variance than the real series.

### Miles

```{r, warnings = FALSE,include=FALSE}
errors<-data.frame(AR=1,MA=1,ERR=1)
k<-1
for (i in 1:5){
  for(j in 1:5){
    model<-arma(RA_Miles_stat,lag=list(ar=c(i),ma=c(j)))
    errors[k,1]<-i
    errors[k,2]<-j
    errors[k,3]<-model$css
    k<-k+1
  }
}
```

```{r, echo=FALSE,fig.align='center', fig.width=5, fig.height=4, fig.cap="Cross validation error against the order of AR and MA processes for the miles time series"}
ggplot(errors)+aes(x=AR,y=ERR,colour=as.factor(MA))+geom_point(size = 2, alpha = 0.5)+ylab("Error") + 
  theme_bw() + ggtitle("Cross validation error against order of the models") + 
  xlab("AR process order")+ ylab("Cross Validation Error")
```

The plot above shows the empirical error of the models. Based on its output we chose a model with AR=1 and MA=2.
For such parameters, we get the following results :

```{r, warnings = FALSE,include=FALSE}
model<-arma(RA_Miles_stat,lag=list(ar=c(1),ma=c(2)))
```

```{r}
qqnorm(model$residuals)
```

```{r, fig.align='center', fig.width=8, fig.height=6, echo = FALSE, warning=FALSE, fig.cap="Fitted ARMA models on the time series Miles"}
#print(" The coefficients of the fitted model are: ")
#model$coef
to_plot<-data.frame(Date=1:length(RA_Miles_stat),Original_Series=RA_Miles_stat,Fitted_Series=model$fitted.values)
Fitted_Series<-rep(NA,length(RA_Miles_stat))
Fitted_Series[length(model$fitted.values)-length(na.omit(model$fitted.values))]<-RA_Miles[length(model$fitted.values)-length(na.omit(model$fitted.values))]
for(i in (length(model$fitted.values)-length(na.omit(model$fitted.values))):(length(RA_Miles_stat))){
  Fitted_Series[i+1]<-Fitted_Series[i]+model$fitted.values[i+1]
}
to_plot<-reshape(to_plot, 
        direction = "long",
        varying = list(names(to_plot)[2:3]),
        v.names = "Value",
        idvar = c("Date"),
        timevar = "Type",
        times = c("Original Series","Fitted Series"))
p1=ggplot(to_plot)+aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  xlab("Monthly Change")+
  ggtitle("Miles Travelled",subtitle = "First difference") + theme_bw()
to_plot<-data.frame(Date=1:length(RA_Miles),Original_Series=RA_Miles,Fitted_Series=Fitted_Series)
to_plot<-reshape(to_plot, 
        direction = "long",
        varying = list(names(to_plot)[2:3]),
        v.names = "Value",
        idvar = c("Date"),
        timevar = "Type",
        times = c("Original Series","Fitted Series"))
p2=ggplot(to_plot)+aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  xlab("Monthly Miles")+
  ggtitle("Miles Travelled",subtitle = "Real Vs. Fitted") + theme_bw()
grid.arrange(p1,p2)
```

Our model reconstruct both the differentiated moving averages of the original series and the moving averages themselves. The only problem is that the predicted values are somewhat lower than the real ones. This time the model also fits quite well the variance of the series.

## Fitting a VAR model

So far we have considered each time series as univariate. We will now fit a VAR model on a vector that contains three time series; Miles, Crude oil prices and Gasoline prices.
The plot below shows side by side the fitted VAR model against the differentiated (i.e. statonnary) time series and the fitted VAR model against the original time series.

```{r, include = FALSE, warning=FALSE}
data_VAR<-na.omit(data5)
data_VAR<-data_VAR[,-c(1,2,5)]
VARselect(data_VAR, lag.max=9, type="const")
model<-VAR(data_VAR,p=1)
#model
```

```{r, fig.align='center', fig.width=12, fig.height=7, echo = FALSE, warning=FALSE, fig.cap="Fitted VAR model on a vector of three time series (On the left, the differentiated time series and on the right the original time series"}
#-- plots for the fitted against the differentiated series
to_plot<-data.frame(na.omit(data5)[,-c(2,5)])

for(i in 1:3){
  to_add<-c(NA,model$varresult[[i]]$fitted.values)
  to_plot<-data.frame(to_plot,to_add)
}

colnames(to_plot)[5:7]<-paste(names(model$varresult),"fitted")

to_plot<-reshape(to_plot, 
        direction = "long",
        varying = list(names(to_plot)[2:7]),
        v.names = "Value",
        idvar = c("Date"),
        timevar = "Type",
        times = colnames(to_plot)[2:7])

to_plot$Variable<-substr(to_plot$Type,1,2)

p1<-ggplot(to_plot[to_plot$Variable=="Ga",])+
  aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  ggtitle("Gasoline prices VAR prediction",subtitle = "Fitted vs Differentiated Series")+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + theme_bw()

p2<-ggplot(to_plot[to_plot$Variable=="Cr",])+
  aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  ggtitle("Crude prices VAR prediction",subtitle = "Fitted vs Differentiated Series")+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + theme_bw()

p3<-ggplot(to_plot[to_plot$Variable=="RA",])+
  aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  ggtitle("Miles VAR prediction",subtitle = "Fitted vs Differentiated Series")+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + theme_bw()

#-- plots for the fitted against the original series 
to_plot<-data.frame(na.omit(data5)[,-c(2,5)])

for(i in 1:3){
  to_add<-c(NA,model$varresult[[i]]$fitted.values)
  to_plot<-data.frame(to_plot,to_add)
}

colnames(to_plot)[5:7]<-paste(names(model$varresult),"fitted")

to_plot[,2:4]<-data3[7:314,c(3,4,6)]
to_plot[1,5:7]<-data3[7,c(3,4,6)]

for(i in 2:nrow(to_plot)){
  to_plot$`Gasoline fitted`[i]<-to_plot$`Gasoline fitted`[i]+to_plot$`Gasoline fitted`[i-1]
  to_plot$`Crude fitted`[i]<-to_plot$`Crude fitted`[i]+to_plot$`Crude fitted`[i-1]
  to_plot$`RA_Miles fitted`[i]<-to_plot$`RA_Miles fitted`[i]+to_plot$`RA_Miles fitted`[i-1]
}

to_plot<-reshape(to_plot, 
        direction = "long",
        varying = list(names(to_plot)[2:7]),
        v.names = "Value",
        idvar = c("Date"),
        timevar = "Type",
        times = colnames(to_plot)[2:7])
to_plot$Variable<-substr(to_plot$Type,1,2)

p4<-ggplot(to_plot[to_plot$Variable=="Ga",])+
  aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  ggtitle("Gasoline prices VAR prediction",subtitle = "Fitted vs original Series")+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + theme_bw()

p5<-ggplot(to_plot[to_plot$Variable=="Cr",])+
  aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  ggtitle("Crude prices VAR prediction",subtitle = "Fitted vs original Series")+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + theme_bw()

p6<-ggplot(to_plot[to_plot$Variable=="RA",])+
  aes(x=Date,y=Value,colour=Type)+
  geom_line()+
  ggtitle("Miles VAR prediction",subtitle = "Fitted vs original Series")+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + theme_bw()

grid.arrange(p1,p4,p2,p5,p3,p6, nrow=3)
```

As expected, we notice a much better visual fit on the right hand side plots due to the fact that we differentiated the time series. By removing the non-stationarity, we are able to capture the structure of the time series. In addition, when we reconstruct the original time series by recovering the differentiation effect after fitting the model on the stationary vector series, we notice that we are able to maintain a similar level of goodness of fit.

In contrast, one should always keep in mind that sometimes the time series at hand might have outliers that may skew the whole analysis. In the case of our vector, there has been some historical events like the 2008 economic crisis in America and the worldwide pandemic in 2020 that induced some anomalies in the series. Deleting such anomalies and repeating the analysis on a cleaner dataset is beyond the scope of this project and, therefore, will remain unexplored.

## Fitting a VAR model with LASSO

Penalized VAR models account for interdependencies and heterogeneities by jointly modeling multiple variables. This is done through a penalization parameter $\lambda$ which reffers to the LASSO model. In this section, we will model the three time series above (i.e. Crude oil prices, Miles traveled and Gasoline prices) using penalized autoregressive models. 

To do so, we will use the nets package which includes a *nets* function that estimates sparse Vector Autoregression models by LASSO.

```{r}
if(!require(nets)) install.packages("nets")
df = data3 %>% 
  as_tibble() %>%
  dplyr::select(Crude, Miles, Gasoline) %>% 
  slice(6:n()) %>% as.matrix()
df[1:10,]
#-- fit Penalized VAR
lamb = 1e-2
model = nets(df, p = 8, lambda = lamb, weights = "adaptive")
```

```{r}
model$A.hat
```

Im not sure what to do here...